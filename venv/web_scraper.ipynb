{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/0m_d_z0n6cn5c9yd2p797w4c0000gn/T/ipykernel_41394/66334634.py:76: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  error_message = detail_soup.find('h1', text=re.compile(r'Page not found', re.IGNORECASE))\n",
      "/var/folders/d6/0m_d_z0n6cn5c9yd2p797w4c0000gn/T/ipykernel_41394/66334634.py:109: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  academic_results_header = detail_soup.find('h4', text='Academic Results')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "School page not found for 'Homestead Senior Secondary College'. Skipping.\n",
      "Data extraction completed. CSV file has been saved.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Constants\n",
    "HEADLESS_MODE = True  # Headless mode for browser\n",
    "START_URL = 'https://www.goodschools.com.au/compare-schools/search/in-victoria/secondary'  # Starting URL for scraping\n",
    "MAX_SCHOOLS = 50  # Set the maximum number of schools to scrape\n",
    "PAGE_LOAD_WAIT_TIME = 5  # Time to wait for pages to load (in seconds)\n",
    "DETAIL_PAGE_WAIT_TIME = 3  # Time to wait for detail pages to load (in seconds)\n",
    "CSV_FILE_NAME = 'victoria_secondary_schools.csv'  # Output CSV file name\n",
    "DEFAULT_VALUE = 'Not provided'  # Default value for missing fields\n",
    "\n",
    "# Example of a local path for chromedriver, commented out for portability\n",
    "# This should be replaced or uncommented only when running on a specific machine\n",
    "# service = Service('/path/to/your/file/venv/chromedriver-mac-arm64/chromedriver') \n",
    "\n",
    "# Set up the WebDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "options = webdriver.ChromeOptions()\n",
    "if HEADLESS_MODE:\n",
    "    options.add_argument('--headless')  # Run in headless mode without GUI\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Set the starting URL\n",
    "driver.get(START_URL)\n",
    "time.sleep(PAGE_LOAD_WAIT_TIME)  # Wait for the page to load\n",
    "\n",
    "# Initialize variables\n",
    "schools_data = []\n",
    "next_page_url = START_URL\n",
    "\n",
    "# Start scraping\n",
    "while len(schools_data) < MAX_SCHOOLS and next_page_url:\n",
    "    # Parse the current page with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all school cards on the page\n",
    "    school_cards = soup.find_all('div', class_='row mt-2 mb-3 pt-3 pb-3 bg-white border-bottom')\n",
    "\n",
    "    for card in school_cards:\n",
    "        try:\n",
    "            # Extract the school name\n",
    "            name_tag = card.find('h5', class_='mb-0 font-weight-bold')\n",
    "            if not name_tag:\n",
    "                continue  # Skip if name not found\n",
    "            name = name_tag.text.strip()\n",
    "\n",
    "            # Extract the school detail page link\n",
    "            school_link_tag = name_tag.find_parent('a')\n",
    "            if school_link_tag is None:\n",
    "                continue  # Skip current card\n",
    "            school_link = school_link_tag['href']\n",
    "\n",
    "            # Extract sector information\n",
    "            info_divs = card.find_all('div', class_='col-md-3 col-6 small')\n",
    "            info_dict = {}\n",
    "            for div in info_divs:\n",
    "                label = div.find('b').text.strip()\n",
    "                value = div.get_text(strip=True).replace(label, '').strip()\n",
    "                info_dict[label] = value\n",
    "            sector = info_dict.get('Sector', '').strip()\n",
    "\n",
    "            # Visit the school's detail page\n",
    "            driver.get(school_link)\n",
    "            time.sleep(DETAIL_PAGE_WAIT_TIME)  # Wait for the page to load\n",
    "\n",
    "            # Parse the detail page\n",
    "            detail_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Check if the page has moved or returns an error\n",
    "            error_message = detail_soup.find('h1', text=re.compile(r'Page not found', re.IGNORECASE))\n",
    "            if error_message:\n",
    "                print(f\"School page not found for '{name}'. Skipping.\")\n",
    "                continue  # Skip this school\n",
    "\n",
    "            # Extract geolocation information\n",
    "            geolocation_tags = detail_soup.find_all('span', class_='map-address load-address')\n",
    "            geolocations = []\n",
    "            postcodes = []\n",
    "            for tag in geolocation_tags:\n",
    "                if 'data-address' in tag.attrs:\n",
    "                    geolocation = tag['data-address'].strip()\n",
    "                    geolocations.append(geolocation)\n",
    "\n",
    "                    # Extract postcode using regex\n",
    "                    postcode_match = re.search(r'(\\d{4})$', geolocation)\n",
    "                    if postcode_match:\n",
    "                        postcode = postcode_match.group(1)\n",
    "                    else:\n",
    "                        postcode = DEFAULT_VALUE\n",
    "                    postcodes.append(postcode)\n",
    "\n",
    "            # Join geolocations and postcodes into comma-separated strings\n",
    "            geolocations_str = ', '.join(geolocations) if geolocations else DEFAULT_VALUE\n",
    "            postcodes_str = ', '.join(postcodes) if postcodes else DEFAULT_VALUE\n",
    "\n",
    "            # Initialize academic result variables with 'Not provided'\n",
    "            scores_40_plus = DEFAULT_VALUE\n",
    "            median_score = DEFAULT_VALUE\n",
    "            vce_completions = DEFAULT_VALUE\n",
    "            vet_completions = DEFAULT_VALUE\n",
    "\n",
    "            # Extract Academic Results and store as separate fields\n",
    "            academic_results_header = detail_soup.find('h4', text='Academic Results')\n",
    "            if academic_results_header:\n",
    "                academic_results_div = academic_results_header.find_parent('div')\n",
    "                result_items = academic_results_div.find_all('p', class_='mb-1')\n",
    "                for item in result_items:\n",
    "                    key_text = item.contents[0].strip().strip(':')\n",
    "                    value_span = item.find('span', class_='font-weight-bold')\n",
    "                    if value_span:\n",
    "                        value_text = value_span.text.strip()\n",
    "\n",
    "                        # Assign values to corresponding variables\n",
    "                        if key_text == 'Scores of 40+':\n",
    "                            scores_40_plus = value_text\n",
    "                        elif key_text == 'Median Score':\n",
    "                            median_score = value_text\n",
    "                        elif key_text == 'Satisfactory completions of VCE':\n",
    "                            vce_completions = value_text\n",
    "                        elif key_text == 'Satisfactory completions of VET':\n",
    "                            vet_completions = value_text\n",
    "\n",
    "            # Add the extracted data to the list\n",
    "            schools_data.append({\n",
    "                'Name': name,\n",
    "                'Sector': sector,\n",
    "                'Geolocation': geolocations_str,\n",
    "                'Postcode': postcodes_str,\n",
    "                'Scores of 40+': scores_40_plus,\n",
    "                'Median Score': median_score,\n",
    "                'Satisfactory completions of VCE': vce_completions,\n",
    "                'Satisfactory completions of VET': vet_completions\n",
    "            })\n",
    "\n",
    "            # Check if we've reached the maximum number of schools\n",
    "            if len(schools_data) >= MAX_SCHOOLS:\n",
    "                break\n",
    "\n",
    "            # Go back to the school listings page\n",
    "            driver.back()\n",
    "            time.sleep(DETAIL_PAGE_WAIT_TIME)  # Wait for the page to load\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing school '{name}': {e}\")\n",
    "            # Go back to the listings page in case of error\n",
    "            driver.back()\n",
    "            time.sleep(DETAIL_PAGE_WAIT_TIME)\n",
    "            continue  # Skip this school\n",
    "\n",
    "    # Check if we've reached the maximum number of schools\n",
    "    if len(schools_data) >= MAX_SCHOOLS:\n",
    "        break\n",
    "\n",
    "    # Find the 'Next' page link\n",
    "    next_page_link = soup.find('a', rel='next')\n",
    "    if next_page_link:\n",
    "        next_page_url = next_page_link['href']\n",
    "        driver.get(next_page_url)\n",
    "        time.sleep(PAGE_LOAD_WAIT_TIME)  # Wait for the page to load\n",
    "    else:\n",
    "        # No more pages to navigate\n",
    "        next_page_url = None\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame and export to CSV\n",
    "df = pd.DataFrame(schools_data)\n",
    "df.fillna(DEFAULT_VALUE, inplace=True)\n",
    "df.to_csv(CSV_FILE_NAME, index=False)\n",
    "print('Data extraction completed. CSV file has been saved.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
